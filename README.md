# üìò Smart Lecture Lens

**Learn Smarter from Your Classrooms using AI**

Smart Lecture Lens is an AI-powered educational platform that transforms traditional lecture videos into interactive learning resources. By leveraging advanced NLP and computer vision, it automatically generates structured summaries, bilingual transcripts, and intelligent quizzes, enabling students to revise efficiently and educators to track learning outcomes.

![Project Status](https://img.shields.io/badge/Status-Active-success)
![License](https://img.shields.io/badge/License-MIT-blue)

---

## üöÄ Key Features

- **üìÇ Multi-Format Upload**: Support for video, audio, and YouTube links via AWS S3.
- **üó£Ô∏è Bilingual Transcription**: High-accuracy speech-to-text in **English** and **Hindi** using OpenAI Whisper/Vosk.
- **üìù AI-Powered Summaries**: Concise, context-aware summaries generated by **Cohere / T5 Transformers**.
- **üñºÔ∏è Optical Character Recognition (OCR)**: Extracts text from lecture slides and whiteboard writings.
- **‚ùì Intelligent Quizzes**: Auto-generates self-assessment quizzes from lecture content.
- **üìä Real-Time Analytics**: Monitor processing status and student performance via WebSockets and interactive dashboards.
- **‚ö° Asynchronous Processing**: Scalable background job handling with **Redis** and **Celery/BullMQ**.

---

## üõ†Ô∏è Tech Stack

### **Frontend**
- **Framework**: [Next.js 15](https://nextjs.org/) (React)
- **Styling**: Tailwind CSS, Radix UI
- **State Management**: Zustand
- **Real-time**: Socket.io Client

### **Backend**
- **Runtime**: Node.js, Express.js
- **Database**: MongoDB (Data), Redis (Queue/Cache)
- **Queue**: BullMQ

### **AI Microservice**
- **Runtime**: Python 3.8+ (FastAPI)
- **ML/AI**: PyTorch, Transformers (Hugging Face), OpenCV, Pytesseract, Vosk

### **DevOps & Infrastructure**
- **Containerization**: Docker, Docker Compose
- **Storage**: AWS S3

---

## üèóÔ∏è System Architecture

```mermaid
graph TD
    Client[Frontend (Next.js)] -->|Uploads/API| API[Backend API (Node.js)]
    API -->|Metadata| DB[(MongoDB)]
    API -->|Jobs| Queue[(Redis Queue)]
    
    subgraph AI Processing
        Worker[Python AI Worker] -->|Fetch Job| Queue
        Worker -->|Speech-to-Text| Whisper[Whisper/Vosk]
        Worker -->|OCR| OCR[OpenCV/Tesseract]
        Worker -->|Summarize| LLM[Cohere/T5]
    end
    
    Worker -->|Update Status| API
    API -->|Real-time Updates| Client
```

---

## üöÄ Getting Started

Follow these instructions to set up the project locally.

### Prerequisites
- **Node.js**: v18+
- **Python**: v3.8+
- **Docker & Docker Compose**
- **FFmpeg** (Required for audio processing)

### 1. Clone the Repository
```bash
git clone <repository-url>
cd Ai_Lecture_lens
```

### 2. Start Infrastructure (Database & Queue)
Use Docker to spin up MongoDB and Redis:
```bash
cd smart-lecture-ai-backend
docker-compose up -d redis
# If you have a mongo container in docker-compose, start it too, otherwise ensure local MongoDB is running.
```

### 3. Backend Setup (Node.js)
```bash
cd smart-lecture-ai-backend/backend

# Install dependencies
npm install

# Configure Environment
cp .env.example .env
# Update .env with your specific keys (AWS, COHERE_API_KEY, MONGO_URI, REDIS_URL)

# Start Server
npm run dev

# Start Worker (in a separate terminal)
npm run worker
```

### 4. AI Service Setup (Python)
```bash
cd smart-lecture-ai-backend/python-ai

# Create virtual environment (Optional but recommended)
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Start FastAPI Server
uvicorn main:app --reload --port 8000
```

### 5. Frontend Setup (Next.js)
```bash
cd ../../  # Navigate back to root/Ai_Lecture_lens

# Install dependencies
npm install

# Configure Environment
# Create .env.local and add:
# NEXT_PUBLIC_API_URL=http://localhost:5000/api
# NEXT_PUBLIC_WS_URL=http://localhost:5000

# Start Application
npm run dev
```
Visit `http://localhost:3000` to view the application.

---

## üìÑ API Reference

### Upload Lecture
`POST /api/lectures/upload`
- **Body**: `multipart/form-data` (video file, title)
- **Response**: `201 Created`

### Get Lecture Status
`GET /api/lectures/:id/status`
- **Response**: `{ status: "processing", step: "transcribing", progress: 45 }`

### Real-time Events (Socket.io)
- Event: `status_update` - Receives live progress updates for the active lecture.

---

## ü§ù Contributing
1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üìÑ License
Distributed under the MIT License. See `LICENSE` for more information.
Project_Report-[M_Project_Report.pdf](https://github.com/user-attachments/files/25138544/M_Project_Report.pdf)

Designe_Thinking on this Project-[Smart Lecture AI-Lens-ppt.pdf](https://github.com/user-attachments/files/25138632/Smart.Lecture.AI-Lens-ppt.pdf)

<img width="1663" height="757" alt="Screenshot 2025-11-01 021139" src="https://github.com/user-attachments/assets/5bd7a405-7e66-4f27-9f4b-b5033b0878c7" /><img width="1693" height="756" alt="Screenshot 2025-11-01 021053" src="https://github.com/user-attachments/assets/f2d2a0ce-da32-456e-b3e8-b96a5e987d71" /><img width="1136" height="609" alt="Screenshot 2025-09-11 131644" src="https://github.com/user-attachments/assets/fb8d8a0b-f38c-4681-82bb-7840300d3c38" />
